<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="keywords" content="remark,remarkjs,markdown,slideshow,presentation" />
    <meta name="description" content="A simple, in-browser, markdown-driven slideshow tool." />
    <title>Remark</title>
    <style>
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body {
        font-family: 'Droid Serif';
      }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        background: #e7e8e2;
        border-radius: 5px;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #777872;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 0.8em;
      }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 1em;
      }
    </style>
  </head>
  <body>
    <textarea id="source">
name: inverse
layout: true
class: center, middle, inverse
---
# How do we know what we see?
21 September 2016
---
layout: false

# Outline

1. How does seeing work?
2. What is visual perception?
3. How do we construct meaning from visual information?
4. How do we think of things we can't see or touch?
---
# How does seeing work?

.pull-left[
**Illusions**

The inverse problem

Visual stimuli

Visual system

]

.pull-right[

![:scale 100%](images/luminance.png)
<center>*Our vision does not work like a camera*

]

---
# Color

.center[![:scale 80%](images/cube.png)]
---
# Color

.center[![:scale 80%](images/brown.png)]

---
# Geometry

.center[![:scale 100%](images/tables.png)]

---

# Geometry

.center[![:scale 100%](images/geometry.png)]

---
# Angles

.center[![:scale 100%](images/angles.png)]

???
The visual world is unknowable: we can't actually see the world like a camera. Purvis believes we evolved perception to behavioral feedback due to operational successes in evolutionary time.

Dale Purvis, "Brains: How they seem to work", 2010
---
# How does seeing work?

.pull-left[
Illusion?

**Visual stimuli**

Visual system

The inverse problem

]

.pull-right[
.center[![:scale 100%](images/electromagnetic-spectrum.png)]
*We see only a tiny portion of the electromagnetic spectrum*
]
---
# How does seeing work?

.pull-left[
Illusion?

**Visual stimuli**

Visual system

The inverse problem

]

.pull-right[
.center[![:scale 100%](images/photon-flux.png)]
*What reaches the eye is not an image but a (chaotic) photon flux*

** Somehow the eye has evolved to make an image on the retina**
]

???
What's different between an eye and a camera?
The heavy lifting is not done by the lens but the cornea.

---
# How does seeing work?

.pull-left[
Illusion?

Visual stimuli

**Visual system**

The inverse problem

]

.pull-right[
.center[![:scale 100%](images/cornea.png)]
*The cornea alters the speed and direction of light rays*
]

???

Since the cornea is heavier than air, when light hits its curved surface, it is refracted.

The lens refines the light a bit earlier.

A primary feature of central vision is the macula lutea (color, blood vasculature difference than the rest of the retina. Fovea has the greatest detail and finest resolution of the eye. To see, we need to make sure we align our eyes by conjugate movement of the two eyes to a point of fixation so that it falls on the fovea (detail and color). Area of greatest sensitivity.

---
# Fovea

.center[![:scale 100%](images/foveal.png)]

???
Ware - arms length, we can resolve 100 points on the head of a padding

But on the edge of our visual field, we can only resolve something the size of a human head
---
# Foveal resolution

.center[![:scale 50%](images/fovea-text.png)]

???

You have to move your eye continually to see a region

Russian physiologist - Alfred Yarbas first to do eye tracking. Contact lens with a mirror on it. Subjects would look at a scene. "What are our eyes doing when looking at a scene normally?"

Making saccadic eye movements as we look at scenes on a continual basis.

Black dots, are where the eye stops. Lines are periods of about 50 ms when the eye is moving. While moving, we're actually blind.

Not random. People look at informational characteristics of a face and less time on in-informative regions. Completely unconscious.
---
# Retina

.center[![:scale 90%](images/retinal-image.png)]

*The retina is the interface between the non-neural and neural parts of the eye

???

It's layered.

Photoreceptor layer (output of ganglion cells)
Fewer photoreceptors as you move toward the fovea
Photoreceptors are at the back of the eye.
Why at the back?
---
# Photoreceptors

.center[![:scale 80%](images/photopigment-rods-cones.png)]

???

 70% of all receptors visual

 Light as electromagnetic radiation in waves is turned into
 nerve impulses that the brain understands.

 Photopigment layer needs to be next to the photoreceptors since they have a high metabolic rate: disks that
 contain photopigment turn over at a fast rate to remove detritus.

 Amacrine and horizontal cells process information in a lateral direction.

---
# Distribution of rods and cones

.center[![:scale 100%](images/dist-rods-cones-retina.png)]

???

Cones centered on fovea.

Presumably, needed both types of photoreceptors to operate over a range of light intensity.
Though, modern humans accustomed to more light.

---
# Photon capture of rods and cones

.center[![:scale 80%](images/retina-rods-cones.png)]

???

Both types efficient at capturing photons.
Rods converge on ganglion cells through intermediary processors. About 100 converge on
a single ganglion cell. About 100 to detect a single flash of light
Cones capture more photons and converge on ganglion cells at about 1 to  1
---
# Luminance range

.center[![:scale 100%](images/luminance-rods-cones.png)]

???

Luminance range is orders of magnitude
---
# Primary visual pathway

.center[![:scale 60%](images/primary-visual-pathway.png)]

???

This is the route from the eye to what we perceive.

Superior colliculus - coordination of eye movements.

Don't know why there is hemisphere crossing. Evolutionary?
Fish tail flip?

---
# How does seeing work?

.pull-left[
Illusion?

Visual stimuli

Visual system

**The inverse problem**

]


.pull-right[

.center[![:scale 70%](images/inverse-problem.jpg)]
<center>*Problem with vision as feature representation (Purvis, 2015)*

]

???

Purvis take on Hubel & Wiesel - neurons in the primary visual pathway of cats and monkeys respond to light stimuli in specific ways
detection of features in the retina play a central role in visual perception.
They discovered that neurons on V1 respond selectively to retinal activation (bars with different orientation, etc.)
Led to they theory that the visual system operates in a straight-forward way of feature
extraction, filtering, processing and analytic combination so that an approximate representation
of physical reality guides our behavior in the world: we see the world as it is.

Purvis suggests this is wrong - starting with "the inverse problem"
Purves, Morgenstern, Wojtach, 2015, "Perception and Reality: Why a Wholly Empirical Paradigm is Needed to Understand Vision", Front. Syst. Neurosci., 18

---
# What is visual perception?

.pull-left[
**Visual thinking**

What is perception?

Feature detection

Attentional tuning of features

Perception

The "what hierarchy"

What and where pathways

]

.pull-right[

"Visual thinking consists of a series of acts of attention, driving eye movements and tuning our pattern finding circuits. These acts of attention are called visual queries."


*Colin Ware, Visual Thinking for Design, 2011, p. 128-129*
]
---
# What does Ware mean by "visual queries"?
.pull-left[
Eye movement is characterized by:
 - Saccadic eye movement (vision is suppressed)
 - Fixations

We perceive during fixations, which last on the order of .1-.2 seconds.

We sample the environment to see and can only see a small amount at a time.
Information critical is not retained.
]
.pull-right[
.center[![:scale 90%](images/visual-query.png)]


]

---
# Illusion: All is in conscious awareness
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/IGQmdoK_ZfY" frameborder="0" allowfullscreen></iframe>

???
.1 seconds seems instantaneous

not the hold the whole world in conscious awareness

Change blindness reflects our small capacity of visual working memory. This is <thead>
reason we need external visual aids. Our attention is displayed by
task-relevant data.
</thead>
---
class: middle, center, inverse

# Seeing is attention

Working memory resources are used to briefly retain
in focal attention those resources that are needed

---
# Feature detection

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/RPv0a9ftu6Y" frameborder="0" allowfullscreen></iframe>



[Hubel and Wiesel cat experiment](https://www.youtube.com/watch?v=IOHayh06LJ4)

???
---
# Features

.pull-left[
Visual thinking

**Feature detection**

Attentional tuning of features

Perception

The "what hierarchy"

What and where pathways

]

.pull-right[

.center[![:scale 100%](images/feature-detection.png)]

While there are 100M visual receptors, there are billions of neurons
simultaneously processing edges, contours, shapes, color, motion and depth.
]

???
- Strongest pop-out affects when a single target objects differs in some feature from all other identical. Degree of contrast important. The more variable the background, the larger the difference. Color, orientation, size, motion & stereoscopic depth. Less than .1 sec.

- No pop out for 2 more features in visual conjunction.

- Learning does not help much in this (hard-wired) phase.

- Detection field  (area around center of the fovea) matters - the differences must be close enough together in visual space.
---
# Color

.pull-left[
Three color opponent channels:
- red-green (middle and long wavelengths)
- yellow-blue (luminance and blue cones)
- black-white (luminance/light intensity)

We are designed to see differences between patches of light: not absolute values.

]
.pull-right[
.center[![:scale 100%](images/ConeMosaics.jpg)]
<center>
Color blind on right

8% of males, 1 % females affected
]

???

- luminescent differences easier to see than chromatic differences.
luminescent channel conveys motion more effectively.
- stereoscopic depth conveyed in luminance channel.
- shape from shading from luminance channel.
- luminance is non-linear. more sensitive to dark grey differences than light differences. background matters.
- 8% of males (& 1 % females) color blind in red0green channel.
- more sensitive to properties of surfaces in the environment rather than light coming from those surfaces.
- Attuned to color for search.
---
# Some consequences for design

1. Closeness in visual space important.
2. Depth is inferred.
3. Color is multi-dimensional and affected by context.
4. For two or more objects in visual query, use different channels. More than 2-3 very challenging. For pop-out, three different steps on each channel - size, orientation, frequencies of motion, etc.
5. Signaling icons should emerge and disappear periodically to reduce habituation.
6. Prior experience leads to pattern of eye movement tendencies. Scanning strategies shaped by the predictability of the environment.
7. Multi-scale visual structure makes search easier.
8. We don't see color peripherally, search tasks might be more effective with shape.
9. Break down tasks visually.
10. It's often easier to re-do than remember.
11. Colors often used symbolically in different cultures.
12. Shape perception mainly based on luminance. (Useful in maps for revealing patterns.)
13. Start with unique hues for color coding. Use no more than a dozen. Background matters.
14. Luminance contrast especially useful for detail and small text.
---
# Attentional tuning of features

.pull-left[
Visual thinking

Feature detection

**Attentional tuning of features**

Perception

The "what hierarchy"

What and where pathways

]

.pull-right[
<center>
## *Patterns are the power of vision*
.center[![:scale 100%](images/patterns.png)]
]

???
Planning and executing eye movements occurs between 1-3 seconds

Scan, plan, pattern test 1-4 per fixation.
---
# Binding

.pull-left[
In this stage, patterns such as continuous contours are constructed.

There is a biased competition for feature processing.

Excitatory signals on neurons in a neighborhood. Inhibition on non-related inputs.

Top-down attentional processes (goals, actions, understanding, etc.) continually
re-link visual and non-visual features. We see what we are looking for. We get
information when we need it.

]

.pull-right[
.center[![:scale 100%](images/contours.png)]

]
---
# Ames illusion
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/gJhyu6nlGt8" frameborder="0" allowfullscreen></iframe>

---
# Perception

.pull-left[
Visual thinking

Feature detection

Attentional tuning of features

**Perception**

The "what hierarchy"

What and where pathways

]

.pull-right[

*Binding together of visual information with non-visual concepts is perception.*
]
---
# Top-down interaction
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UCy-Lc6hfFA" frameborder="0" allowfullscreen></iframe>

???

The brain is a distributed computer with no central conductor.
---
# Different people can perceive the world differently

<center>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/jexnhNfOzHg" frameborder="0" allowfullscreen></iframe>

https://www.wired.com/2015/02/science-one-agrees-color-dress/
---
# The "What hierarchy"

.center[![:scale 100%](images/what-hierarchy.png)]

???

 Information reduction to objects. Visual working memory. Three objects in attention at a time. Small capacity of visual working memory.
---
# What and where pathways

.pull-left[
Visual thinking

Feature detection

Attentional tuning of features

Perception

The "what hierarchy"

**What and where pathways**

]

.pull-right[

Where pathway - the lateral interparietal area markes locations of recent fixations
and inhibits the tendency to revisit.

.center[![:scale 100%](images/where-pathway.png)]
]
---
# Planning and reading
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/VFIZDZwdf-0" frameborder="0" allowfullscreen></iframe>

---
# Challenges to reading
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/TwNNij89qro" frameborder="0" allowfullscreen></iframe>

---
# How do we construct meaning?


.center[![:scale 70%](images/dog.png)]


---
# Some final thoughts on visual perception

1. The visual world is unknowable: we don't see the world like a camera.
2. Cognition is distributed - top-down and bottom-up processes have an effect on perception.
3. Graphics are designed to communicate. Consider the visual task and allow that to drive design.
4. The power of visual thinking is pattern matching -- but patterns aren't limited to the visual - perceptions linked to actions are response patterns.
5. In pattern learning, V1 & V2 are universally the same, V4 is idiosyncratic based on experience.
---
# How do we think of things we can't touch?

.left-column[
**Embodied cognition?**

Space

Metaphor

Visual imagery

Performance on cognitive tasks

Mirror neurons

]

.right-column[

*Cognition depends on aspects of our bodies and not just our brains.
We offload our cognitive processing onto the environment and distribute
across our physical, social and cultural environment. (Suchman, 1987)*

]
---
# Embodied cognition

*Thought is based on the way our bodies work*

<iframe width="560" height="315" src="https://www.youtube.com/embed/Eu-9rpJITY8" frameborder="0" allowfullscreen></iframe>

[Lakoff - Frames and metaphor](https://www.youtube.com/watch?v=S_CWBjyIERY)

???

Historically, in philosophy of mind & cognitive science considered the body to be peripheral to thinking

---
# Space

In English, we have three frames of reference.

.pull-left[
- Object
  - The chair is in front of the table.
- Viewer (egocentric)
  - The chair is behind the table.
- Absolute
  - The chair is to the North of the table.
]

.pull-right[
.center[![:scale 100%](images/chair-table.jpg)]
]

Egocentric - up, sideways, towards-away
---
# Metaphor

Metaphors are patterns of patterns.

Spatial concepts like "front", "back", "up", "down" are articulated
in terms of our body's position and movement.

But, we use them to talk about abstract things:
- Happy is up, sad is down
- push a button/limit (abstract object)
- Halloween is coming up (moving objects; future events are up)
- Weeks ahead

Properties and functions of objects matter: in/on bowl/plate

![:scale 20%](images/apple.jpg)

---
# Visual imagery

<iframe width="560" height="315" src="https://www.youtube.com/embed/OTEa1jwJbqU" frameborder="0" allowfullscreen></iframe>

When you visualize an image, your eyes move a pattern that reflects that image (skip to min 6)
---
# Performance on cognitive tasks

We can perform cognitive tasks better when using our bodies or parts of the environment around us (Donald, 1991).

Perception of space is fundamentally about perception of action potential -- the linking perception to action.
- Softball players who are hitting better see the ball a bigger.
- Does ability influence perception or is there response bias (judgement bias from memory)?
---
# Mirror Neurons
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/t0pwKzTRG5E" frameborder="0" allowfullscreen></iframe>
---
class: middle, center, inverse
## How do we think of things we can't see or touch?
Perhaps, we borrow from sensory and motor representations as we interact with the world
---

# References

Donald, M. (1991). Origins of the Modern Mind: Three Stages in the Evolution of Culture and Cognition. Harvard University Press.

Lakoff, G., & M. Johnson. (1999). Philosophy in the Flesh: The Embodied Mind and its Challenge to Western Thought, New York: Basic Books.

Purves, D. (2010). Brains: How They Seem to Work. Ft Press.

Purves D, Morgenstern Y, & Wojtach WT. (2015). Perception and reality: Why a wholly empirical paradigm is needed to understand vision. Front. Syst. Neurosci. 9:156.

Suchman, Lucy (1987). Plans and Situated Actions: The Problem of Human-machine Communication. Cambridge: Cambridge University Press.

Ware, C. (2010). Visual Thinking for Design. Morgan Kaufmann.

---
class: center, middle, inverse
# End
    </textarea>
     <script src="assets/remark-latest.min.js">
    <script>
      var hljs = remark.highlighter.engine;
    </script>
    <script src="assets/remark.language.js"></script>
    <script>
      remark.macros.scale = function(percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
      }
    </script>

    <script>
      var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          click: true,
          highlightLines: true
        }) ;
    </script>
  </body>
</html>
